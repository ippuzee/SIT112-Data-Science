{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6139240-3b34-461a-80ef-4320364f26db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A. Imports\n",
    "You need to import the following libraries. Install the libraries using \"conda install ... or pip install ...\" if they have not been installed on your machine. For example you can install google api python client by executing \"conda install google-api-python-client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14131785-ec21-4364-96fc-7d930999ba5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-api-python-client module not found. Installing...\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.116.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m436.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.16.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Downloading google_api_python_client-2.116.0-py2.py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m788.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.16.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.2/135.2 kB\u001b[0m \u001b[31m653.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uritemplate, rsa, protobuf, httplib2, cachetools, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.2 google-api-core-2.16.1 google-api-python-client-2.116.0 google-auth-2.27.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.62.0 httplib2-0.22.0 protobuf-4.25.2 rsa-4.9 uritemplate-4.1.1\n",
      "langdetect module not found. Installing...\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m600.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in /home/ipiot/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=b41bc4000e7a27390150c0e69a0f6d3eb8270810f6e88a4c1dec04a311309e06\n",
      "  Stored in directory: /home/ipiot/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "textblob module not found. Installing...\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m373.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /home/ipiot/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/ipiot/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ipiot/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /home/ipiot/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "prettytable module not found. Installing...\n",
      "Collecting prettytable\n",
      "  Downloading prettytable-3.9.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ipiot/anaconda3/lib/python3.11/site-packages (from prettytable) (0.2.5)\n",
      "Downloading prettytable-3.9.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.9.0\n",
      "All required modules are installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ipiot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "# The following lines should install all libraries you need - you can install the libraries manually if the script did not work \n",
    "required_modules = ['pandas', 'seaborn', 'matplotlib', 'google-api-python-client', 'datetime', 'configparser', 'nltk', 'langdetect', 'textblob', 'prettytable', 'tabulate', 'numpy']\n",
    "for module in required_modules:\n",
    "    try:\n",
    "        importlib.import_module(module)\n",
    "    except ImportError:\n",
    "        print(f\"{module} module not found. Installing...\")\n",
    "        subprocess.check_call(['pip', 'install', module])\n",
    "\n",
    "print(\"All required modules are installed.\")\n",
    "\n",
    "# import the installed libraries ...\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "from textblob import TextBlob\n",
    "import calendar\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from prettytable import PrettyTable\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62382a9-a4eb-4079-88c7-86ff18f981ba",
   "metadata": {},
   "source": [
    "### B. Settings\n",
    "This section specifies the settings for connecting to the YouTube API and collecting the data about YouTube videos and their corresponding comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3914f598-971c-499a-9909-6776720a2221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIDEOS_FILE = \"videos.csv\"\n",
    "COMMENTS_FILE = \"comments.csv\"\n",
    "CREDENTIALS_FILE = 'credentials.ini'\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2023, 1, 1)\n",
    "KEYWORDS =['coronavirus', 'covid', 'covid-19', 'pandemic']\n",
    "# You can use functin get_channel_info() to extract the channel ID of a sample video from a news publisher ...\n",
    "CHANNELS = {\n",
    "    'UCXIJgqnII2ZOINSWNOGFThA' : 'Fox News',\n",
    "    'UC16niRr50-MSBwiO3YDb3RA' : 'BBC News',\n",
    "    'UCupvZG-5ko_eiXAupbDfxWw' : 'CNN',\n",
    "    'UCaXkIU1QidjPwiAYu6GcHjg' : 'MSNBC'\n",
    "}\n",
    "MAX_VIDEOS = 50 # the maximum number of video that should be returned for each request. Acceptable values are 0 to 50\n",
    "QUERY= f\"intitle:{','.join(KEYWORDS)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010ce18-ccee-459e-97bd-73d13e5ad0ec",
   "metadata": {},
   "source": [
    "### C. Load the credentials for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3dfcd6-0ae1-41bc-82c0-f9368d071e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials():\n",
    "    try:\n",
    "        config = ConfigParser(interpolation=None)\n",
    "        config.read(CREDENTIALS_FILE)\n",
    "        developer_key = config.get('credentials_youtube', 'developer_key', fallback=None)\n",
    "        service_name = config.get('credentials_youtube', 'youtube_api_service_name', fallback=None)\n",
    "        service_version = config.get('credentials_youtube', 'youtube_api_version', fallback=None)\n",
    "        if not developer_key or not service_name or not service_version:\n",
    "            raise ValueError(\"Invalid credentials file\")\n",
    "\n",
    "        return {\n",
    "            'developer_key' : developer_key,\n",
    "            'service_name' : service_name,\n",
    "            'service_version' : service_version\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Failed to load credentials: {}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03189f46-940d-4822-976e-691e9b0cf503",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D. Extract the channel_id and channel_title of a sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8503383a-2975-4339-a921-e8b547735f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function takes a video ID and a YouTube Object and returns the video's channel ID\n",
    "# See Section H (call the functions) to learn how to use this function\n",
    "def get_channel_info(video_id, youtube):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    channel_id = response['items'][0]['snippet']['channelId']\n",
    "    channel_title = response['items'][0]['snippet']['channelTitle']\n",
    "    return channel_id, channel_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f3ff2-52f2-4395-8e08-bf4a7c4288e3",
   "metadata": {},
   "source": [
    "### E. Search for the videos from the channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7669b972-7e4c-4248-b705-0c1d2123ca59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_channel_videos(channel_ids, start_date, end_date, query, video_categories={}, max_videos=10):\n",
    "    df_list = []\n",
    "    for channel_id in channel_ids: \n",
    "        print(f\"-> collecting videos for channel: {CHANNELS[channel_id]}\")\n",
    "        try:\n",
    "            request = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                type='video',\n",
    "                channelId=channel_id,\n",
    "                maxResults=max_videos, # specifies the maximum number of items that should be returned in the result set. Acceptable values are 0 to 50, inclusive.\n",
    "                q=query,\n",
    "                publishedAfter=start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                publishedBefore=end_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            )\n",
    "            response = request.execute()\n",
    "            videos = response['items']\n",
    "            data = []\n",
    "            for video in videos:\n",
    "                video_id = video['id']['videoId']\n",
    "                video_details = youtube.videos().list(\n",
    "                    part=\"snippet,statistics,contentDetails\",\n",
    "                    id=video_id\n",
    "                ).execute()\n",
    "                video_data = {\n",
    "                    'video_id' : video_id,\n",
    "                    'channel_id' : channel_id,\n",
    "                    'video_title': video_details['items'][0]['snippet']['title'],\n",
    "                    'channel_title': video_details['items'][0]['snippet']['channelTitle'],\n",
    "                    'category_name': video_categories.get(str(video_details['items'][0]['snippet']['categoryId']), 'Unknown'),\n",
    "                    'live_upcoming_none' : video_details['items'][0]['snippet']['liveBroadcastContent'],\n",
    "                    'view_count': video_details['items'][0]['statistics'].get('viewCount', 0),\n",
    "                    'like_count': video_details['items'][0]['statistics'].get('likeCount', 0),\n",
    "                    'dislike_count': video_details['items'][0]['statistics'].get('dislikeCount', 0),\n",
    "                    'comment_count': video_details['items'][0]['statistics'].get('commentCount', 0),\n",
    "                    'published_at': video_details['items'][0]['snippet']['publishedAt'],\n",
    "                    'tags': ','.join(video_details['items'][0]['snippet'].get('tags', [])),\n",
    "                    'duration': video_details['items'][0]['contentDetails'].get('duration', ''),\n",
    "                    'definition': video_details['items'][0]['contentDetails'].get('definition', 'unknown'),\n",
    "                    'caption': video_details['items'][0]['contentDetails'].get('caption', 'false'),\n",
    "                    'thumbnail' : video_details['items'][0]['snippet']['thumbnails']['default'].get('url'),\n",
    "                    'url': 'https://www.youtube.com/watch?v={}'.format(video_id)\n",
    "                }\n",
    "                data.append(video_data)\n",
    "            df = pd.DataFrame(data)\n",
    "            df_list.append(df)\n",
    "        except HttpError as e:\n",
    "            print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred:\\n{str(e)}')\n",
    "    df_concatenated = pd.concat(df_list, axis=0)\n",
    "    df_concatenated.to_csv(VIDEOS_FILE, mode='w', index=False)\n",
    "    return df_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d7761-5a07-4c44-a7ed-c4b6302aa0ac",
   "metadata": {},
   "source": [
    "### F. Retrieve comments for a list of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2e63bd-8072-4275-b854-2055d0fb10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_comments():\n",
    "    videos = pd.read_csv(VIDEOS_FILE)\n",
    "    video_ids = videos['video_id'].tolist()\n",
    "    df_list =[]    \n",
    "    # Loop through all the video IDs and retrieve the comments\n",
    "    for video_id in video_ids:\n",
    "        print(f\"-> collecting comments for video: {video_id}\")\n",
    "        comments_list = []\n",
    "        try:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                textFormat='plainText'\n",
    "            ).execute()\n",
    "\n",
    "            # Loop through all the comments and extract the relevant information\n",
    "            for item in response['items']:\n",
    "                comment_id = item['snippet']['topLevelComment']['id']\n",
    "                comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comment_author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "                comment_date = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "                reply_count = item['snippet']['totalReplyCount']\n",
    "                comments_list.append([video_id, comment_id, comment_text, comment_author, comment_date, like_count, None])\n",
    "                \n",
    "                if reply_count > 0:\n",
    "                    # Retrieve the replies to the top-level comment\n",
    "                    reply_response = youtube.comments().list(\n",
    "                        part='snippet',\n",
    "                        parentId=comment_id,\n",
    "                        textFormat='plainText'\n",
    "                    ).execute()\n",
    "                    \n",
    "                    # Loop through all the replies and extract the relevant information\n",
    "                    for reply_item in reply_response['items']:\n",
    "                        reply_id = reply_item['id']\n",
    "                        reply_text = reply_item['snippet']['textDisplay']\n",
    "                        reply_author = reply_item['snippet']['authorDisplayName']\n",
    "                        reply_date = reply_item['snippet']['publishedAt']\n",
    "                        reply_like_count = reply_item['snippet']['likeCount']\n",
    "                        comments_list.append([video_id, reply_id, reply_text, reply_author, reply_date, reply_like_count, comment_id])\n",
    "\n",
    "        except HttpError as error:\n",
    "            if error.resp.status == 403:\n",
    "                print(f'Comments are disabled for video ID {video_id}. Skipping...')\n",
    "            else:\n",
    "                raise error\n",
    "        \n",
    "        df = pd.DataFrame(comments_list, columns=['video_id', 'comment_id', 'comment_text', 'comment_author', 'comment_date', 'comment_like_count', 'parent_comment_id'])\n",
    "        df_list.append(df)\n",
    "    df_concatenated = pd.concat(df_list, axis=0)\n",
    "    df_concatenated.to_csv(COMMENTS_FILE, mode='w', index=False)\n",
    "    return df_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53640a66-446e-470c-9b7d-928367dcfee3",
   "metadata": {},
   "source": [
    "### G. Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4282dba5-9feb-4860-be39-efac070d9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(VIDEOS_FILE, COMMENTS_FILE, stopwords):\n",
    "    # Load videos data\n",
    "    videos = pd.read_csv(VIDEOS_FILE)\n",
    "\n",
    "    # Clean videos data\n",
    "    videos['video_title'] = videos['video_title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if isinstance(x, str) else x) # remove punctuation\n",
    "    videos['video_title'] = videos['video_title'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x) # remove digits\n",
    "    videos['video_title'] = videos['video_title'].apply(lambda x: x.lower() if isinstance(x, str) else x) # convert to lowercase\n",
    "\n",
    "    # Save cleaned videos data to new CSV file, replacing the existing file\n",
    "    videos.to_csv(VIDEOS_FILE, index=False)\n",
    "\n",
    "    # Load comments data\n",
    "    comments = pd.read_csv(COMMENTS_FILE)\n",
    "\n",
    "    # Clean comments data\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if isinstance(x, str) else x)  # remove punctuation\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)  # remove digits\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: x.lower() if isinstance(x, str) else x)  # convert to lowercase\n",
    "\n",
    "    # Remove duplicates\n",
    "    comments = comments.drop_duplicates()\n",
    "\n",
    "    # Remove rows with missing comment_text\n",
    "    comments = comments.dropna(subset=['comment_text'])\n",
    "\n",
    "    # Filter out comments that are not in English\n",
    "    try:\n",
    "        comments = comments[comments['comment_text'].apply(lambda x: langdetect.detect(x) == 'en')]\n",
    "    except langdetect.LangDetectException as e:\n",
    "        print(f\"non-english comment skipped ... {e}\")\n",
    "    # Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    # Save cleaned comments data to new CSV file, replacing the existing file\n",
    "    comments.to_csv(COMMENTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e72618-4248-458a-b753-bded1ce6f0ff",
   "metadata": {},
   "source": [
    "## H. Call the functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466fb70c-e28c-4b70-8a0b-a74f6fcf2bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> collecting videos for channel: Fox News\n",
      "-> collecting videos for channel: BBC News\n",
      "-> collecting videos for channel: CNN\n",
      "-> collecting videos for channel: MSNBC\n",
      "-> Videos have been Collected ---------------------------\n",
      "-> collecting comments for video: d1eEWihvwpQ\n",
      "-> collecting comments for video: lydWQoqDNp8\n",
      "-> collecting comments for video: u9tLP9y6mPQ\n",
      "-> collecting comments for video: jXHikITwlng\n",
      "-> collecting comments for video: lp2czs3U8Qk\n",
      "-> collecting comments for video: ACLU0OxWisE\n",
      "-> collecting comments for video: -r_C-QsWEak\n",
      "-> collecting comments for video: zwX5z_BvaN4\n",
      "-> collecting comments for video: eT47ulzfOMo\n",
      "-> collecting comments for video: 6G_-qe0iMBs\n",
      "-> collecting comments for video: Z5cG6m0fYXY\n",
      "-> collecting comments for video: 9LBUutWsfBc\n",
      "-> collecting comments for video: _FUiPbzoGSQ\n",
      "-> collecting comments for video: DZLQRA5XwPY\n",
      "-> collecting comments for video: kUZgGQwYL7c\n",
      "-> collecting comments for video: lJcjFGt9eQM\n",
      "-> collecting comments for video: wqX2PuPTOhM\n",
      "-> collecting comments for video: xhpqX1NbusQ\n",
      "-> collecting comments for video: ebtpY4jZ9ds\n",
      "-> collecting comments for video: G6HbxcCb-7g\n",
      "-> collecting comments for video: EZ2TPymcdL4\n",
      "-> collecting comments for video: H9N9M9IjXD0\n",
      "-> collecting comments for video: zFWn5IVLYQk\n",
      "-> collecting comments for video: gkR0jipzPRA\n",
      "-> collecting comments for video: VdK77bhAwO0\n",
      "-> collecting comments for video: Oei9_SRfaSw\n",
      "-> collecting comments for video: 76J7SqZXG1g\n",
      "-> collecting comments for video: aSBWlKC7reM\n",
      "-> collecting comments for video: JNhc67mSmEM\n",
      "-> collecting comments for video: mbzsRZ1gX1M\n",
      "-> collecting comments for video: DbiCwYhZnGo\n",
      "-> collecting comments for video: qLy8ISicMDI\n",
      "-> collecting comments for video: ob5_NvJz1io\n",
      "-> collecting comments for video: bf4WRdPnf4I\n",
      "-> collecting comments for video: EE1iX5HKELA\n",
      "-> collecting comments for video: 0pkwjVhq230\n",
      "-> collecting comments for video: vX2KxYmJfjE\n",
      "-> collecting comments for video: fVpbMvz2auY\n",
      "-> collecting comments for video: J0Kavkyn46U\n",
      "-> collecting comments for video: _HGTvjC0xrw\n",
      "-> collecting comments for video: Kx5I3fM6Rws\n",
      "-> collecting comments for video: dmpbWUt338I\n",
      "-> collecting comments for video: IwpM0n8Hspg\n",
      "-> collecting comments for video: hIu9BqQva3I\n",
      "-> collecting comments for video: 8506miurvpQ\n",
      "-> collecting comments for video: HnjvX8oLBdo\n",
      "-> collecting comments for video: hIpCSt6NUJg\n",
      "-> collecting comments for video: 7voTUuVT5i4\n",
      "-> collecting comments for video: TSyJcWRLBZ8\n",
      "-> collecting comments for video: tBjYLo0xZzI\n",
      "-> collecting comments for video: YK_gq3PC7TE\n",
      "-> collecting comments for video: RHFV0XcJxww\n",
      "-> collecting comments for video: ArFQdvF8vDE\n",
      "-> collecting comments for video: cIhak7--M8Q\n",
      "-> collecting comments for video: P2y1sIr913Y\n",
      "-> collecting comments for video: UNZRvU923Jo\n",
      "-> collecting comments for video: LYQLr45rtZk\n",
      "-> collecting comments for video: hHcY4Vw532Y\n",
      "-> collecting comments for video: dc11uZ7_PnU\n",
      "-> collecting comments for video: mmSJQul260g\n",
      "-> collecting comments for video: NAV3CmA5rBU\n",
      "-> collecting comments for video: NwPwj45CAIc\n",
      "-> collecting comments for video: 9nrZwjcjS1A\n",
      "-> collecting comments for video: tSCWs85mye8\n",
      "-> collecting comments for video: 0sKlL4k2-sc\n",
      "-> collecting comments for video: gtf9j06EOvo\n",
      "-> collecting comments for video: F5L-7p3v4GI\n",
      "-> collecting comments for video: Eh_7aSruRqw\n",
      "-> collecting comments for video: RSZoDTvgEr8\n",
      "-> collecting comments for video: 21MIvkk7Imc\n",
      "-> collecting comments for video: aQ6LERWK0T0\n",
      "-> collecting comments for video: 8dTelszbObM\n",
      "-> collecting comments for video: v8lVBHgucfs\n",
      "-> collecting comments for video: qEj4MYC__Zo\n",
      "-> collecting comments for video: 0NTsBJLU1nk\n",
      "-> collecting comments for video: oKX_E7adVwg\n",
      "-> collecting comments for video: LuESjGI-_gM\n",
      "Comments are disabled for video ID LuESjGI-_gM. Skipping...\n",
      "-> collecting comments for video: UcxHL8LHI_A\n",
      "-> collecting comments for video: 3vudfCtGWhE\n",
      "-> collecting comments for video: HlEFrbLeDks\n",
      "-> collecting comments for video: 6CajGH3Q5RU\n",
      "Comments are disabled for video ID 6CajGH3Q5RU. Skipping...\n",
      "-> collecting comments for video: xKW7l-u0OKA\n",
      "-> collecting comments for video: E0S3Zcg9ZJo\n",
      "-> collecting comments for video: CuneZ1LhbQ0\n",
      "-> collecting comments for video: lHVNvFX8Bfg\n",
      "-> collecting comments for video: JtxsuIW7nrY\n",
      "-> collecting comments for video: JP7OtUAVAjE\n",
      "-> collecting comments for video: SimARiFYWCQ\n",
      "-> collecting comments for video: VgPZtTeEG5k\n",
      "-> collecting comments for video: STgRHkB-vx4\n",
      "-> collecting comments for video: P4SYSRp00uE\n",
      "-> collecting comments for video: SLhGsTuE0ik\n",
      "-> collecting comments for video: LP9a6cvfVeU\n",
      "-> collecting comments for video: bYQjssb7xdk\n",
      "-> collecting comments for video: v2r1cnTUnkE\n",
      "-> collecting comments for video: glm0tMRHTkQ\n",
      "-> collecting comments for video: 7fRmK51-Q5o\n",
      "-> collecting comments for video: WE9gvJ8ou7g\n",
      "-> collecting comments for video: W3ypPfbTyZs\n",
      "-> collecting comments for video: uGVvsn7Q7mY\n",
      "-> collecting comments for video: ZtkSYY9Z2fE\n",
      "-> collecting comments for video: rgpSC__Nxso\n",
      "-> collecting comments for video: pwB_wfqQlcM\n",
      "-> collecting comments for video: sZBMcqLKOqo\n",
      "-> collecting comments for video: 3mfSSi-J7kI\n",
      "-> collecting comments for video: SfWtBPbE9so\n",
      "-> collecting comments for video: bDBRxJTOKxM\n",
      "-> collecting comments for video: GEARtTBHXEw\n",
      "-> collecting comments for video: OV1Vuv9zfzI\n",
      "-> collecting comments for video: eEUqCxP5Lvc\n",
      "-> collecting comments for video: J0Cpmx4DS1c\n",
      "-> collecting comments for video: S-uFhQcYxv8\n",
      "-> collecting comments for video: _2OwQOPGKfg\n",
      "-> collecting comments for video: udERg05aUxI\n",
      "-> collecting comments for video: A71lfXrQlxU\n",
      "-> collecting comments for video: lm7Xi3cw6dY\n",
      "-> collecting comments for video: j_mmRsUWpYo\n",
      "-> collecting comments for video: KZn1yulnc1U\n",
      "-> collecting comments for video: 5RunFCEZh_4\n",
      "-> collecting comments for video: HuBkxYY883k\n",
      "-> collecting comments for video: OOvENoZMmK4\n",
      "-> collecting comments for video: FCUpvgp4RUo\n",
      "-> collecting comments for video: v_vDjR5zBDk\n",
      "-> collecting comments for video: ATndga6ZMSs\n",
      "-> collecting comments for video: Tak_fw_0QbM\n",
      "-> collecting comments for video: -szt0u2X0WI\n",
      "-> collecting comments for video: R2ZETF515Ec\n",
      "-> collecting comments for video: OYrROwpHECk\n",
      "-> collecting comments for video: 02Wi5GSkdNk\n",
      "-> collecting comments for video: 7qvADOmHCUQ\n",
      "-> collecting comments for video: qqCZC-gFA6A\n",
      "-> collecting comments for video: AXUxqGS1Jkw\n",
      "-> collecting comments for video: X8O8EBLj-0c\n",
      "-> collecting comments for video: iVo5j9mZq7g\n",
      "-> collecting comments for video: QQEQH59p17U\n",
      "-> collecting comments for video: i4fx-hLaYok\n",
      "-> collecting comments for video: WBRaGT9nqs4\n",
      "-> collecting comments for video: girPw3j5OJs\n",
      "-> collecting comments for video: VQqFr7_Y5NU\n",
      "-> collecting comments for video: gl24Ts6JjZ8\n",
      "-> collecting comments for video: aCifvImcjAY\n",
      "-> collecting comments for video: sXpIrDGXeTg\n",
      "-> collecting comments for video: RX9r5ORKdp0\n",
      "-> collecting comments for video: XfbvLn3DgV8\n",
      "-> collecting comments for video: YoCfs-9AiiQ\n",
      "-> collecting comments for video: QsH3Y93dzLA\n",
      "-> collecting comments for video: o_UaQaokSOc\n",
      "-> collecting comments for video: BTAub1YO4k0\n",
      "-> collecting comments for video: uo43AbYlK2I\n",
      "-> collecting comments for video: A_rMCaYkk68\n",
      "-> collecting comments for video: cbbrlKl4Fak\n",
      "-> collecting comments for video: 2WgNLO67Z18\n",
      "-> collecting comments for video: 5BQ8l8grQ4g\n",
      "-> collecting comments for video: qlxsHz1YfMA\n",
      "-> collecting comments for video: QwGfY5CJJZ0\n",
      "-> collecting comments for video: YpbKKD1NAlE\n",
      "-> collecting comments for video: Tm_KWLzEhNE\n",
      "-> collecting comments for video: orWhiVs8Kjg\n",
      "-> collecting comments for video: 7T6iH0OJ_Sk\n",
      "-> collecting comments for video: szORuX9KVc4\n",
      "-> collecting comments for video: c8TltWFwKuA\n",
      "-> collecting comments for video: BPnWazsVOok\n",
      "-> collecting comments for video: -2h2bQYO-0E\n",
      "-> collecting comments for video: sBmQam6kY0Q\n",
      "-> collecting comments for video: W6NLnZnYSLs\n",
      "-> collecting comments for video: wXzrP7dBb1g\n",
      "-> collecting comments for video: 2gq5FV4rEgQ\n",
      "-> collecting comments for video: Ev7JetAKVTo\n",
      "-> collecting comments for video: 2kEM0XZf4Bo\n",
      "-> collecting comments for video: 2QIB9E7IoVM\n",
      "-> collecting comments for video: uCQkIdxZykE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> collecting comments for video: pEVJUgPZfBE\n",
      "-> collecting comments for video: E7dIDW6icDw\n",
      "-> collecting comments for video: kNFwBPmynA4\n",
      "-> collecting comments for video: 0tSACr4-fW8\n",
      "-> collecting comments for video: 3PunhcmRlYc\n",
      "-> collecting comments for video: JrL_nB4dyn0\n",
      "-> collecting comments for video: Fwg9n01bdro\n",
      "-> collecting comments for video: tBbKfmeFk4E\n",
      "-> collecting comments for video: yLyDCMjdbVQ\n",
      "-> collecting comments for video: 3x5-AbdB-Bk\n",
      "-> collecting comments for video: YOu3er_zpAo\n",
      "-> collecting comments for video: tSDrVpLQPTQ\n",
      "-> collecting comments for video: vnOB7EEmGII\n",
      "-> collecting comments for video: VZfYsPMn5ow\n",
      "-> collecting comments for video: 5QQBUqzU5II\n",
      "-> collecting comments for video: kjQqRsAd2ig\n",
      "-> collecting comments for video: n9PSwD62b_s\n",
      "-> collecting comments for video: vzOLgfU9ypQ\n",
      "-> collecting comments for video: QwIZdQVJAL4\n",
      "-> collecting comments for video: YMXjl8i_T5c\n",
      "-> collecting comments for video: cMcCyGORMcE\n",
      "-> collecting comments for video: qv0WRX_Uuxo\n",
      "-> collecting comments for video: Dfa-cQc1X74\n",
      "-> collecting comments for video: EeRsyWCM5gE\n",
      "-> collecting comments for video: cCYOqx8qho8\n",
      "-> collecting comments for video: x_ZX24j9jcE\n",
      "-> collecting comments for video: oG_iTyhVH_A\n",
      "-> collecting comments for video: rtzAgHFHyNw\n",
      "-> collecting comments for video: UJWkExxcTd0\n",
      "-> Comments have been Collected -------------------------\n",
      "non-english comment skipped ... No features in text.\n",
      "-> Data Cleaning has been Completed ---------------------\n"
     ]
    }
   ],
   "source": [
    "# Read the developer_key, service_name, and service_version from credentials.ini \n",
    "credentials = load_credentials()\n",
    "\n",
    "# Build a youtube object using the build function\n",
    "youtube = build(credentials['service_name'], credentials['service_version'],developerKey=credentials['developer_key'])\n",
    "\n",
    "# Exctract the video categories\n",
    "response = youtube.videoCategories().list(part='snippet', regionCode='UK').execute()\n",
    "VIDEO_CATEGORIES = {category['id']: category['snippet']['title'] for category in response['items']}\n",
    "\n",
    "# The following line shows how to extract the channel_id and channel_title of a video with video_id \"OOrW82pHlMQ\"\n",
    "# channel_id, channel_title = get_channel_info('OOrW82pHlMQ', youtube)\n",
    "# print(f'{channel_id}, {channel_title}')\n",
    "\n",
    "#  ------------  Get the data -------------------------------------\n",
    "get_channel_videos(list(CHANNELS.keys()), START_DATE, END_DATE, QUERY, VIDEO_CATEGORIES, max_videos = MAX_VIDEOS).head()\n",
    "print (\"-> Videos have been Collected ---------------------------\")\n",
    "get_videos_comments().head()\n",
    "print (\"-> Comments have been Collected -------------------------\")\n",
    "# -------------- Clean the data -----------------------------------\n",
    "clean_data(VIDEOS_FILE, COMMENTS_FILE, stopwords)\n",
    "print (\"-> Data Cleaning has been Completed ---------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2920b276a3ebce4ce72dc8ffba4d2570b82a96386913ad33f11b89ba088715fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
